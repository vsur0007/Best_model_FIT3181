{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_all(seed=1234)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_all(seed=1234)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"\n",
    "    This class manages and preprocesses a simple text dataset for a sentence classification task.\n",
    "\n",
    "    Attributes:\n",
    "        verbose (bool): Controls verbosity for printing information during data processing.\n",
    "        max_sentence_len (int): The maximum length of a sentence in the dataset.\n",
    "        str_questions (list): A list to store the string representations of the questions in the dataset.\n",
    "        str_labels (list): A list to store the string representations of the labels in the dataset.\n",
    "        numeral_labels (list): A list to store the numerical representations of the labels in the dataset.\n",
    "        maxlen (int): Maximum length for padding sequences. Sequences longer than this length will be truncated,\n",
    "            and sequences shorter than this length will be padded with zeros. Defaults to 50.\n",
    "        numeral_data (list): A list to store the numerical representations of the questions in the dataset.\n",
    "        random_state (int): Seed value for random number generation to ensure reproducibility.\n",
    "            Set this value to a specific integer to reproduce the same random sequence every time. Defaults to 6789.\n",
    "        random (np.random.RandomState): Random number generator object initialized with the given random_state.\n",
    "            It is used for various random operations in the class.\n",
    "\n",
    "    Methods:\n",
    "        maybe_download(dir_name, file_name, url, verbose=True):\n",
    "            Downloads a file from a given URL if it does not exist in the specified directory.\n",
    "            The directory and file are created if they do not exist.\n",
    "\n",
    "        read_data(dir_name, file_names):\n",
    "            Reads data from files in a directory, preprocesses it, and computes the maximum sentence length.\n",
    "            Each file is expected to contain rows in the format \"<label>:<question>\".\n",
    "            The labels and questions are stored as string representations.\n",
    "\n",
    "        manipulate_data():\n",
    "            Performs data manipulation by tokenizing, numericalizing, and padding the text data.\n",
    "            The questions are tokenized and converted into numerical sequences using a tokenizer.\n",
    "            The sequences are padded or truncated to the maximum sequence length.\n",
    "\n",
    "        train_valid_test_split(train_ratio=0.9):\n",
    "            Splits the data into training, validation, and test sets based on a given ratio.\n",
    "            The data is randomly shuffled, and the specified ratio is used to determine the size of the training set.\n",
    "            The string questions, numerical data, and numerical labels are split accordingly.\n",
    "            TensorFlow `Dataset` objects are created for the training and validation sets.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=True, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose=True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "\n",
    "    def read_data(self, dir_name, file_names):\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str = row.split(\":\")\n",
    "                    label, question = row_str[0], row_str[1]\n",
    "                    question = question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len = len(self.str_questions[-1])\n",
    "\n",
    "        # turns labels into numbers\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes = le.classes_\n",
    "        self.num_classes = len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions and corresponding labels... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(self.str_labels[0:5])\n",
    "\n",
    "    def manipulate_data(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "        token_ids = []\n",
    "        num_seqs = []\n",
    "        for text in self.str_questions:  # iterate over the list of text\n",
    "          text_seqs = self.tokenizer.tokenize(str(text))  # tokenize each text individually\n",
    "          # Convert tokens to IDs\n",
    "          token_ids = self.tokenizer.convert_tokens_to_ids(text_seqs)\n",
    "          # Convert token IDs to a tensor of indices using your word2idx mapping\n",
    "          seq_tensor = torch.LongTensor(token_ids)\n",
    "          num_seqs.append(seq_tensor)  # append the tensor for each sequence\n",
    "\n",
    "        # Pad the sequences and create a tensor\n",
    "        if num_seqs:\n",
    "          self.numeral_data = pad_sequence(num_seqs, batch_first=True)  # Pads to max length of the sequences\n",
    "          self.num_sentences, self.max_seq_len = self.numeral_data.shape\n",
    "\n",
    "    def train_valid_test_split(self, train_ratio=0.8, test_ratio = 0.1):\n",
    "        train_size = int(self.num_sentences*train_ratio) +1\n",
    "        test_size = int(self.num_sentences*test_ratio) +1\n",
    "        valid_size = self.num_sentences - (train_size + test_size)\n",
    "        data_indices = list(range(self.num_sentences))\n",
    "        random.shuffle(data_indices)\n",
    "        self.train_str_questions = [self.str_questions[i] for i in data_indices[:train_size]]\n",
    "        self.train_numeral_labels = self.numeral_labels[data_indices[:train_size]]\n",
    "        train_set_data = self.numeral_data[data_indices[:train_size]]\n",
    "        train_set_labels = self.numeral_labels[data_indices[:train_size]]\n",
    "        train_set_labels = torch.from_numpy(train_set_labels)\n",
    "        train_set = torch.utils.data.TensorDataset(train_set_data, train_set_labels)\n",
    "        self.test_str_questions = [self.str_questions[i] for i in data_indices[-test_size:]]\n",
    "        self.test_numeral_labels = self.numeral_labels[data_indices[-test_size:]]\n",
    "        test_set_data = self.numeral_data[data_indices[-test_size:]]\n",
    "        test_set_labels = self.numeral_labels[data_indices[-test_size:]]\n",
    "        test_set_labels = torch.from_numpy(test_set_labels)\n",
    "        test_set = torch.utils.data.TensorDataset(test_set_data, test_set_labels)\n",
    "        self.valid_str_questions = [self.str_questions[i] for i in data_indices[train_size:-test_size]]\n",
    "        self.valid_numeral_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
    "        valid_set_data = self.numeral_data[data_indices[train_size:-test_size]]\n",
    "        valid_set_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
    "        valid_set_labels = torch.from_numpy(valid_set_labels)\n",
    "        valid_set = torch.utils.data.TensorDataset(valid_set_data, valid_set_labels)\n",
    "        self.train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "        self.valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "\n",
    "dm = DataManager()\n",
    "dm.read_data(\"data/\", [\"train_2000.label\"])\n",
    "dm.manipulate_data()\n",
    "dm.train_valid_test_split(train_ratio=0.8, test_ratio = 0.1)\n",
    "for x, y in dm.train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BaseTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.criterion = criterion  #the loss function\n",
    "        self.optimizer = optimizer  #the optimizer\n",
    "        self.train_loader = train_loader  #the train loader\n",
    "        self.val_loader = val_loader  #the valid loader\n",
    "\n",
    "    #the function to train the model in many epochs\n",
    "    def fit(self, num_epochs):\n",
    "        self.num_batches = len(self.train_loader)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            train_loss, train_accuracy = self.train_one_epoch()\n",
    "            val_loss, val_accuracy = self.validate_one_epoch()\n",
    "            print(\n",
    "                f'{self.num_batches}/{self.num_batches} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy*100:.4f}% \\\n",
    "                - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy*100:.4f}%')\n",
    "\n",
    "    #train in one epoch, return the train_acc, train_loss\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device).long()  # Cast labels to LongTensor\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_accuracy = correct / total\n",
    "        train_loss = running_loss / self.num_batches\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "\n",
    "    #evaluate on a loader and return the loss and accuracy\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device).long()  # Cast labels to LongTensor\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        loss = loss / len(self.val_loader)\n",
    "        return loss, accuracy\n",
    "\n",
    "    #return the val_acc, val_loss, be called at the end of each epoch\n",
    "    def validate_one_epoch(self):\n",
    "      val_loss, val_accuracy = self.evaluate(self.val_loader)\n",
    "      return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 9154.52 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from datasets import Dataset\n",
    "\n",
    "model_name = \"bert-base-uncased\"  # BERT or any similar model\n",
    "\n",
    "# Tokenize input and prepare model inputs\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": dm.str_questions, \"label\": dm.numeral_labels})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length= 36)\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function splits the BERT dataset `dataset` into three BERT datasets for training, valid, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(dataset, train_ratio=0.8, test_ratio = 0.1):\n",
    "    num_sentences = len(dataset)\n",
    "    train_size = int(num_sentences*train_ratio) +1\n",
    "    test_size = int(num_sentences*test_ratio) +1\n",
    "    valid_size = num_sentences - (train_size + test_size)\n",
    "    train_set = dataset[:train_size]\n",
    "    train_set = Dataset.from_dict(train_set)\n",
    "    train_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_set = dataset[-test_size:]\n",
    "    test_set = Dataset.from_dict(test_set)\n",
    "    test_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    valid_set = dataset[train_size:-test_size]\n",
    "    valid_set = Dataset.from_dict(valid_set)\n",
    "    valid_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)\n",
    "    return train_loader, test_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, valid_loader = train_valid_test_split(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the class `PrefixTuningForClassification` for the prefix prompt fine-tuning. We first load a pre-trained BERT model specified by `model_name`. The parameter `prefix_length` specifies the length of the prefix prompts we add to the pre-trained BERT model. Specifically, given the input batch `[batch_size, seq_len]`, we input to the embedding layer of the pre-trained BERT model to obtain `[batch_size, seq_len, embed_size]`. We create the prefix prompts $P$ of the size `[prefix_length, embed_size]` and concatenate to the embeddings from the pre-trained BERT to obtain `[batch_size, seq_len + prefix_length, embed_size]`. This concatenation tensor will then be fed to the encoder layers of the pre-trained BERT layer to obtain the last `[batch_size, seq_len + prefix_length, embed_size]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take mean across the seq_len to obtain `[batch_size, embed_size]` on which we can build up a linear layer for making predictions. Please note that **the parameters to tune include the prefix prompts $P$** and **the output linear layer**, and you should freeze the parameters of the BERT pre-trained model. Moreover, your code should cover the edge case when `prefix_length=None`. In this case, we do not insert any prefix prompts and we only do fine-tuning for the output linear layer on top.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixTuningForClassification(nn.Module):\n",
    "    def __init__(self, model_name, prefix_length=None, data_manager=None):\n",
    "        super(PrefixTuningForClassification, self).__init__()\n",
    "\n",
    "        # Load the pretrained transformer model (BERT-like model)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        self.prefix_length = prefix_length\n",
    "        self.num_classes = data_manager.num_classes\n",
    "\n",
    "        # Freeze BERT parameters (only fine-tuning the prompt and classifier layers)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # If prefix_length is specified, create prefix embeddings\n",
    "        if self.prefix_length is not None:\n",
    "            self.prefix_embeddings = nn.Parameter(\n",
    "                torch.randn(self.prefix_length, self.hidden_size)\n",
    "            )\n",
    "        \n",
    "        # Linear layer for classification\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the embeddings from BERT\n",
    "        embedding_output = self.model.embeddings(input_ids)\n",
    "\n",
    "        # If prefix_length is specified, prepend the prefix to the embedding\n",
    "        if self.prefix_length is not None:\n",
    "            batch_size = embedding_output.size(0)\n",
    "            # Repeat the prefix for each sample in the batch\n",
    "            prefix = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            # Concatenate prefix with embeddings along sequence length dimension\n",
    "            embedding_output = torch.cat((prefix, embedding_output), dim=1)\n",
    "\n",
    "            # Extend the attention mask for the prefix tokens (with value 1, since they should not be masked)\n",
    "            prefix_mask = torch.ones((batch_size, self.prefix_length), dtype=attention_mask.dtype).to(device)\n",
    "            attention_mask = torch.cat((prefix_mask, attention_mask), dim=1)\n",
    "\n",
    "        # Cast attention_mask to float (if not already) and expand dimensions\n",
    "        attention_mask = attention_mask.to(dtype=torch.float)\n",
    "        attention_mask = attention_mask[:, None, None, :]  # Expand mask for multi-head attention\n",
    "\n",
    "        # Pass through the BERT encoder\n",
    "        encoder_outputs = self.model.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get the last hidden state from the encoder\n",
    "        sequence_output = encoder_outputs.last_hidden_state  # [batch_size, seq_len + prefix_length, hidden_size]\n",
    "\n",
    "        # Take the mean across sequence length (global average pooling)\n",
    "        pooled_output = torch.mean(sequence_output, dim=1)  # [batch_size, hidden_size]\n",
    "\n",
    "        # Pass through the classifier for prediction\n",
    "        logits = self.classifier(pooled_output)  # [batch_size, num_classes]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following `FineTunedBaseTrainer` to train the prompt fine-tuning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedBaseTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.criterion = criterion  #the loss function\n",
    "        self.optimizer = optimizer  #the optimizer\n",
    "        self.train_loader = train_loader  #the train loader\n",
    "        self.val_loader = val_loader  #the valid loader\n",
    "\n",
    "    #the function to train the model in many epochs\n",
    "    def fit(self, num_epochs):\n",
    "        self.num_batches = len(self.train_loader)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            train_loss, train_accuracy = self.train_one_epoch()\n",
    "            val_loss, val_accuracy = self.validate_one_epoch()\n",
    "            print(\n",
    "                f'{self.num_batches}/{self.num_batches} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy*100:.4f}% \\\n",
    "                - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy*100:.4f}%')\n",
    "\n",
    "    #train in one epoch, return the train_acc, train_loss\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for batch in self.train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(input_ids= input_ids, attention_mask= attention_mask)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_accuracy = correct / total\n",
    "        train_loss = running_loss / self.num_batches\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "    #evaluate on a loader and return the loss and accuracy\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = self.model(input_ids= input_ids, attention_mask= attention_mask)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        loss = loss / len(self.val_loader)\n",
    "        return loss, accuracy\n",
    "\n",
    "    #return the val_acc, val_loss, be called at the end of each epoch\n",
    "    def validate_one_epoch(self):\n",
    "      val_loss, val_accuracy = self.evaluate(self.val_loader)\n",
    "      return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare and train the prefix-prompt tuning model. In addition, you need to be patient with this model because it might converge slowly with many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_tuning_model = PrefixTuningForClassification(model_name=\"bert-base-uncased\", prefix_length=20, data_manager=dm).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 - train_loss: 1.6873 - train_accuracy: 25.4841%                 - val_loss: 0.8231 - val_accuracy: 28.7879%\n",
      "Epoch 2/100\n",
      "26/26 - train_loss: 1.5871 - train_accuracy: 38.6633%                 - val_loss: 0.7774 - val_accuracy: 32.3232%\n",
      "Epoch 3/100\n",
      "26/26 - train_loss: 1.6114 - train_accuracy: 46.4085%                 - val_loss: 0.8169 - val_accuracy: 32.8283%\n",
      "Epoch 4/100\n",
      "26/26 - train_loss: 1.4183 - train_accuracy: 48.2823%                 - val_loss: 0.7836 - val_accuracy: 53.0303%\n",
      "Epoch 5/100\n",
      "26/26 - train_loss: 1.3544 - train_accuracy: 51.3429%                 - val_loss: 0.8449 - val_accuracy: 55.0505%\n",
      "Epoch 6/100\n",
      "26/26 - train_loss: 1.2763 - train_accuracy: 56.9644%                 - val_loss: 0.7435 - val_accuracy: 60.1010%\n",
      "Epoch 7/100\n",
      "26/26 - train_loss: 1.1752 - train_accuracy: 59.7751%                 - val_loss: 0.7205 - val_accuracy: 60.6061%\n",
      "Epoch 8/100\n",
      "26/26 - train_loss: 1.1207 - train_accuracy: 62.2736%                 - val_loss: 0.6775 - val_accuracy: 62.6263%\n",
      "Epoch 9/100\n",
      "26/26 - train_loss: 1.0581 - train_accuracy: 64.8969%                 - val_loss: 0.6865 - val_accuracy: 61.6162%\n",
      "Epoch 10/100\n",
      "26/26 - train_loss: 1.0217 - train_accuracy: 65.8339%                 - val_loss: 0.6201 - val_accuracy: 69.1919%\n",
      "Epoch 11/100\n",
      "26/26 - train_loss: 0.9571 - train_accuracy: 68.0200%                 - val_loss: 0.6135 - val_accuracy: 64.1414%\n",
      "Epoch 12/100\n",
      "26/26 - train_loss: 0.8981 - train_accuracy: 70.2061%                 - val_loss: 0.5154 - val_accuracy: 67.6768%\n",
      "Epoch 13/100\n",
      "26/26 - train_loss: 0.9013 - train_accuracy: 70.5184%                 - val_loss: 0.5542 - val_accuracy: 71.7172%\n",
      "Epoch 14/100\n",
      "26/26 - train_loss: 0.8335 - train_accuracy: 72.8295%                 - val_loss: 0.4391 - val_accuracy: 72.2222%\n",
      "Epoch 15/100\n",
      "26/26 - train_loss: 0.8078 - train_accuracy: 74.8907%                 - val_loss: 0.4326 - val_accuracy: 76.2626%\n",
      "Epoch 16/100\n",
      "26/26 - train_loss: 0.8167 - train_accuracy: 76.0775%                 - val_loss: 0.4558 - val_accuracy: 76.2626%\n",
      "Epoch 17/100\n",
      "26/26 - train_loss: 0.8165 - train_accuracy: 75.0156%                 - val_loss: 0.3745 - val_accuracy: 74.7475%\n",
      "Epoch 18/100\n",
      "26/26 - train_loss: 0.7445 - train_accuracy: 76.7021%                 - val_loss: 0.3187 - val_accuracy: 77.7778%\n",
      "Epoch 19/100\n",
      "26/26 - train_loss: 0.6809 - train_accuracy: 77.3891%                 - val_loss: 0.3618 - val_accuracy: 77.2727%\n",
      "Epoch 20/100\n",
      "26/26 - train_loss: 0.6354 - train_accuracy: 79.4503%                 - val_loss: 0.3225 - val_accuracy: 82.3232%\n",
      "Epoch 21/100\n",
      "26/26 - train_loss: 0.6381 - train_accuracy: 80.9494%                 - val_loss: 0.3334 - val_accuracy: 82.8283%\n",
      "Epoch 22/100\n",
      "26/26 - train_loss: 0.6196 - train_accuracy: 81.3866%                 - val_loss: 0.2810 - val_accuracy: 82.3232%\n",
      "Epoch 23/100\n",
      "26/26 - train_loss: 0.5908 - train_accuracy: 82.1362%                 - val_loss: 0.2856 - val_accuracy: 82.3232%\n",
      "Epoch 24/100\n",
      "26/26 - train_loss: 0.5585 - train_accuracy: 83.6977%                 - val_loss: 0.2514 - val_accuracy: 88.3838%\n",
      "Epoch 25/100\n",
      "26/26 - train_loss: 0.5291 - train_accuracy: 83.9475%                 - val_loss: 0.2166 - val_accuracy: 88.8889%\n",
      "Epoch 26/100\n",
      "26/26 - train_loss: 0.5027 - train_accuracy: 84.8220%                 - val_loss: 0.2029 - val_accuracy: 90.4040%\n",
      "Epoch 27/100\n",
      "26/26 - train_loss: 0.4834 - train_accuracy: 85.3841%                 - val_loss: 0.1747 - val_accuracy: 88.8889%\n",
      "Epoch 28/100\n",
      "26/26 - train_loss: 0.4619 - train_accuracy: 86.8207%                 - val_loss: 0.1627 - val_accuracy: 90.4040%\n",
      "Epoch 29/100\n",
      "26/26 - train_loss: 0.4676 - train_accuracy: 86.6958%                 - val_loss: 0.1631 - val_accuracy: 88.3838%\n",
      "Epoch 30/100\n",
      "26/26 - train_loss: 0.4212 - train_accuracy: 88.6321%                 - val_loss: 0.1484 - val_accuracy: 89.8990%\n",
      "Epoch 31/100\n",
      "26/26 - train_loss: 0.4142 - train_accuracy: 87.3829%                 - val_loss: 0.1256 - val_accuracy: 90.9091%\n",
      "Epoch 32/100\n",
      "26/26 - train_loss: 0.4037 - train_accuracy: 87.8826%                 - val_loss: 0.1270 - val_accuracy: 88.3838%\n",
      "Epoch 33/100\n",
      "26/26 - train_loss: 0.3739 - train_accuracy: 89.7564%                 - val_loss: 0.1220 - val_accuracy: 92.9293%\n",
      "Epoch 34/100\n",
      "26/26 - train_loss: 0.3674 - train_accuracy: 89.6939%                 - val_loss: 0.0922 - val_accuracy: 92.9293%\n",
      "Epoch 35/100\n",
      "26/26 - train_loss: 0.3451 - train_accuracy: 90.1312%                 - val_loss: 0.0894 - val_accuracy: 92.9293%\n",
      "Epoch 36/100\n",
      "26/26 - train_loss: 0.3536 - train_accuracy: 90.0687%                 - val_loss: 0.0918 - val_accuracy: 91.4141%\n",
      "Epoch 37/100\n",
      "26/26 - train_loss: 0.3210 - train_accuracy: 91.3804%                 - val_loss: 0.0747 - val_accuracy: 93.9394%\n",
      "Epoch 38/100\n",
      "26/26 - train_loss: 0.3196 - train_accuracy: 92.5047%                 - val_loss: 0.0673 - val_accuracy: 93.4343%\n",
      "Epoch 39/100\n",
      "26/26 - train_loss: 0.3364 - train_accuracy: 91.7552%                 - val_loss: 0.0697 - val_accuracy: 94.4444%\n",
      "Epoch 40/100\n",
      "26/26 - train_loss: 0.3119 - train_accuracy: 90.7558%                 - val_loss: 0.0646 - val_accuracy: 94.9495%\n",
      "Epoch 41/100\n",
      "26/26 - train_loss: 0.2867 - train_accuracy: 92.1299%                 - val_loss: 0.0805 - val_accuracy: 93.4343%\n",
      "Epoch 42/100\n",
      "26/26 - train_loss: 0.2693 - train_accuracy: 93.1293%                 - val_loss: 0.0521 - val_accuracy: 94.4444%\n",
      "Epoch 43/100\n",
      "26/26 - train_loss: 0.3267 - train_accuracy: 93.3791%                 - val_loss: 0.0639 - val_accuracy: 93.9394%\n",
      "Epoch 44/100\n",
      "26/26 - train_loss: 0.2662 - train_accuracy: 93.1293%                 - val_loss: 0.0416 - val_accuracy: 95.9596%\n",
      "Epoch 45/100\n",
      "26/26 - train_loss: 0.2484 - train_accuracy: 93.8788%                 - val_loss: 0.0513 - val_accuracy: 95.4545%\n",
      "Epoch 46/100\n",
      "26/26 - train_loss: 0.2315 - train_accuracy: 94.2536%                 - val_loss: 0.0433 - val_accuracy: 95.4545%\n",
      "Epoch 47/100\n",
      "26/26 - train_loss: 0.2396 - train_accuracy: 94.1287%                 - val_loss: 0.0437 - val_accuracy: 95.4545%\n",
      "Epoch 48/100\n",
      "26/26 - train_loss: 0.2470 - train_accuracy: 93.3167%                 - val_loss: 0.0327 - val_accuracy: 96.9697%\n",
      "Epoch 49/100\n",
      "26/26 - train_loss: 0.2241 - train_accuracy: 93.8164%                 - val_loss: 0.0383 - val_accuracy: 95.4545%\n",
      "Epoch 50/100\n",
      "26/26 - train_loss: 0.2030 - train_accuracy: 95.1280%                 - val_loss: 0.0235 - val_accuracy: 96.9697%\n",
      "Epoch 51/100\n",
      "26/26 - train_loss: 0.2135 - train_accuracy: 94.5034%                 - val_loss: 0.0277 - val_accuracy: 97.4747%\n",
      "Epoch 52/100\n",
      "26/26 - train_loss: 0.1948 - train_accuracy: 95.2530%                 - val_loss: 0.0250 - val_accuracy: 96.9697%\n",
      "Epoch 53/100\n",
      "26/26 - train_loss: 0.1926 - train_accuracy: 95.6902%                 - val_loss: 0.0203 - val_accuracy: 97.9798%\n",
      "Epoch 54/100\n",
      "26/26 - train_loss: 0.1984 - train_accuracy: 95.6277%                 - val_loss: 0.0194 - val_accuracy: 97.9798%\n",
      "Epoch 55/100\n",
      "26/26 - train_loss: 0.1846 - train_accuracy: 95.8151%                 - val_loss: 0.0179 - val_accuracy: 96.4646%\n",
      "Epoch 56/100\n",
      "26/26 - train_loss: 0.1765 - train_accuracy: 95.6902%                 - val_loss: 0.0165 - val_accuracy: 97.9798%\n",
      "Epoch 57/100\n",
      "26/26 - train_loss: 0.1725 - train_accuracy: 95.3779%                 - val_loss: 0.0174 - val_accuracy: 97.9798%\n",
      "Epoch 58/100\n",
      "26/26 - train_loss: 0.1614 - train_accuracy: 95.6277%                 - val_loss: 0.0146 - val_accuracy: 96.9697%\n",
      "Epoch 59/100\n",
      "26/26 - train_loss: 0.1562 - train_accuracy: 96.8770%                 - val_loss: 0.0119 - val_accuracy: 96.9697%\n",
      "Epoch 60/100\n",
      "26/26 - train_loss: 0.1718 - train_accuracy: 96.0650%                 - val_loss: 0.0160 - val_accuracy: 97.4747%\n",
      "Epoch 61/100\n",
      "26/26 - train_loss: 0.1726 - train_accuracy: 95.8151%                 - val_loss: 0.0086 - val_accuracy: 97.9798%\n",
      "Epoch 62/100\n",
      "26/26 - train_loss: 0.1785 - train_accuracy: 95.8151%                 - val_loss: 0.0098 - val_accuracy: 97.9798%\n",
      "Epoch 63/100\n",
      "26/26 - train_loss: 0.1520 - train_accuracy: 95.5653%                 - val_loss: 0.0129 - val_accuracy: 95.9596%\n",
      "Epoch 64/100\n",
      "26/26 - train_loss: 0.1532 - train_accuracy: 96.9394%                 - val_loss: 0.0095 - val_accuracy: 97.9798%\n",
      "Epoch 65/100\n",
      "26/26 - train_loss: 0.1323 - train_accuracy: 97.2517%                 - val_loss: 0.0088 - val_accuracy: 97.9798%\n",
      "Epoch 66/100\n",
      "26/26 - train_loss: 0.1275 - train_accuracy: 96.9394%                 - val_loss: 0.0080 - val_accuracy: 97.4747%\n",
      "Epoch 67/100\n",
      "26/26 - train_loss: 0.1243 - train_accuracy: 97.3766%                 - val_loss: 0.0078 - val_accuracy: 97.9798%\n",
      "Epoch 68/100\n",
      "26/26 - train_loss: 0.1280 - train_accuracy: 96.8145%                 - val_loss: 0.0073 - val_accuracy: 98.4848%\n",
      "Epoch 69/100\n",
      "26/26 - train_loss: 0.1165 - train_accuracy: 97.7514%                 - val_loss: 0.0063 - val_accuracy: 98.4848%\n",
      "Epoch 70/100\n",
      "26/26 - train_loss: 0.1253 - train_accuracy: 97.6265%                 - val_loss: 0.0057 - val_accuracy: 98.4848%\n",
      "Epoch 71/100\n",
      "26/26 - train_loss: 0.1130 - train_accuracy: 97.5016%                 - val_loss: 0.0058 - val_accuracy: 97.9798%\n",
      "Epoch 72/100\n",
      "26/26 - train_loss: 0.1130 - train_accuracy: 97.5640%                 - val_loss: 0.0050 - val_accuracy: 97.9798%\n",
      "Epoch 73/100\n",
      "26/26 - train_loss: 0.1449 - train_accuracy: 97.2517%                 - val_loss: 0.0072 - val_accuracy: 97.9798%\n",
      "Epoch 74/100\n",
      "26/26 - train_loss: 0.1451 - train_accuracy: 95.7527%                 - val_loss: 0.0071 - val_accuracy: 98.9899%\n",
      "Epoch 75/100\n",
      "26/26 - train_loss: 0.1065 - train_accuracy: 97.8763%                 - val_loss: 0.0060 - val_accuracy: 98.9899%\n",
      "Epoch 76/100\n",
      "26/26 - train_loss: 0.1067 - train_accuracy: 97.6265%                 - val_loss: 0.0064 - val_accuracy: 98.9899%\n",
      "Epoch 77/100\n",
      "26/26 - train_loss: 0.0983 - train_accuracy: 98.2511%                 - val_loss: 0.0069 - val_accuracy: 98.9899%\n",
      "Epoch 78/100\n",
      "26/26 - train_loss: 0.0986 - train_accuracy: 97.8139%                 - val_loss: 0.0065 - val_accuracy: 98.9899%\n",
      "Epoch 79/100\n",
      "26/26 - train_loss: 0.1257 - train_accuracy: 97.7514%                 - val_loss: 0.0059 - val_accuracy: 98.9899%\n",
      "Epoch 80/100\n",
      "26/26 - train_loss: 0.1128 - train_accuracy: 97.1268%                 - val_loss: 0.0050 - val_accuracy: 98.4848%\n",
      "Epoch 81/100\n",
      "26/26 - train_loss: 0.0969 - train_accuracy: 97.1893%                 - val_loss: 0.0050 - val_accuracy: 98.9899%\n",
      "Epoch 82/100\n",
      "26/26 - train_loss: 0.0875 - train_accuracy: 97.9388%                 - val_loss: 0.0055 - val_accuracy: 98.9899%\n",
      "Epoch 83/100\n",
      "26/26 - train_loss: 0.0880 - train_accuracy: 98.1262%                 - val_loss: 0.0055 - val_accuracy: 98.9899%\n",
      "Epoch 84/100\n",
      "26/26 - train_loss: 0.0866 - train_accuracy: 97.8139%                 - val_loss: 0.0053 - val_accuracy: 98.9899%\n",
      "Epoch 85/100\n",
      "26/26 - train_loss: 0.0789 - train_accuracy: 98.5009%                 - val_loss: 0.0049 - val_accuracy: 98.9899%\n",
      "Epoch 86/100\n",
      "26/26 - train_loss: 0.0892 - train_accuracy: 97.5016%                 - val_loss: 0.0047 - val_accuracy: 98.9899%\n",
      "Epoch 87/100\n",
      "26/26 - train_loss: 0.0838 - train_accuracy: 98.1886%                 - val_loss: 0.0043 - val_accuracy: 98.4848%\n",
      "Epoch 88/100\n",
      "26/26 - train_loss: 0.0870 - train_accuracy: 97.8763%                 - val_loss: 0.0045 - val_accuracy: 98.4848%\n",
      "Epoch 89/100\n",
      "26/26 - train_loss: 0.0774 - train_accuracy: 98.6259%                 - val_loss: 0.0041 - val_accuracy: 98.9899%\n",
      "Epoch 90/100\n",
      "26/26 - train_loss: 0.0985 - train_accuracy: 98.1886%                 - val_loss: 0.0037 - val_accuracy: 98.9899%\n",
      "Epoch 91/100\n",
      "26/26 - train_loss: 0.0867 - train_accuracy: 97.6265%                 - val_loss: 0.0037 - val_accuracy: 97.9798%\n",
      "Epoch 92/100\n",
      "26/26 - train_loss: 0.0761 - train_accuracy: 98.0637%                 - val_loss: 0.0046 - val_accuracy: 97.9798%\n",
      "Epoch 93/100\n",
      "26/26 - train_loss: 0.0937 - train_accuracy: 98.1262%                 - val_loss: 0.0041 - val_accuracy: 98.9899%\n",
      "Epoch 94/100\n",
      "26/26 - train_loss: 0.0994 - train_accuracy: 97.3142%                 - val_loss: 0.0042 - val_accuracy: 97.9798%\n",
      "Epoch 95/100\n",
      "26/26 - train_loss: 0.0740 - train_accuracy: 98.7508%                 - val_loss: 0.0041 - val_accuracy: 98.9899%\n",
      "Epoch 96/100\n",
      "26/26 - train_loss: 0.0734 - train_accuracy: 98.2511%                 - val_loss: 0.0036 - val_accuracy: 98.9899%\n",
      "Epoch 97/100\n",
      "26/26 - train_loss: 0.0716 - train_accuracy: 98.1262%                 - val_loss: 0.0028 - val_accuracy: 98.4848%\n",
      "Epoch 98/100\n",
      "26/26 - train_loss: 0.0672 - train_accuracy: 98.4385%                 - val_loss: 0.0029 - val_accuracy: 98.9899%\n",
      "Epoch 99/100\n",
      "26/26 - train_loss: 0.0649 - train_accuracy: 98.4385%                 - val_loss: 0.0031 - val_accuracy: 98.9899%\n",
      "Epoch 100/100\n",
      "26/26 - train_loss: 0.0672 - train_accuracy: 98.5009%                 - val_loss: 0.0026 - val_accuracy: 98.4848%\n"
     ]
    }
   ],
   "source": [
    "# Optimizer: tune the classifier and prefix embeddings if prefix_length is provided\n",
    "if prefix_tuning_model.prefix_length is not None:\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(prefix_tuning_model.classifier.parameters()) + [prefix_tuning_model.prefix_embeddings], lr=1e-3\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(prefix_tuning_model.classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Trainer setup\n",
    "trainer = FineTunedBaseTrainer(model=prefix_tuning_model, criterion=criterion, optimizer=optimizer, train_loader=train_loader, val_loader=valid_loader)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
